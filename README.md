ðŸ“Š Paul Graham RAG Evaluator â€” DeepEval Test Runner

This repository contains an enhanced DeepEval test runner designed to evaluate Retrieval-Augmented Generation (RAG) performance on a Paul Graham essay dataset.
It uses Azure OpenAI (GPT-5) as the judge model and runs multiple metrics (Answer Relevancy + Contextual Precision) on a batch of test cases loaded from CSV.

The script includes:

âœ” Automatic loading of .env keys

âœ” Custom DeepEval LLM wrapper for Azure OpenAI

âœ” Automatic CSV input â†’ test case conversion

âœ” JSON / delimiter parsing for retrieval contexts

âœ” Sanity-check failure test (ensures judge is not overly lenient)

âœ” Full diagnostic printing for each metric

âœ” Automatic CSV export of results

âœ” Compact failure summary

âœ” Warning if all tests pass (likely misconfiguration)
