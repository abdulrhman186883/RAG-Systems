ðŸ“Š Paul Graham RAG Evaluator â€” DeepEval Test Runner * paul_Deepeval.py *

This repository contains an enhanced DeepEval test runner designed to evaluate Retrieval-Augmented Generation (RAG) performance on a Paul Graham essay dataset.
It uses Azure OpenAI (GPT-5) as the judge model and runs multiple metrics (Answer Relevancy + Contextual Precision) on a batch of test cases loaded from CSV.

The script includes:

âœ” Automatic loading of .env keys

âœ” Custom DeepEval LLM wrapper for Azure OpenAI

âœ” Automatic CSV input â†’ test case conversion

âœ” JSON / delimiter parsing for retrieval contexts

âœ” Sanity-check failure test (ensures judge is not overly lenient)

âœ” Full diagnostic printing for each metric

âœ” Automatic CSV export of results

âœ” Compact failure summary

âœ” Warning if all tests pass (likely misconfiguration)



ðŸ“š Paul Graham RAG Chatbot * my_langChainPaulGhram.py *

A complete pipeline: scraping â†’ parsing â†’ chunking â†’ embeddings â†’ Chroma â†’ RAG Q&A

This project builds a local RAG (Retrieval Augmented Generation) chatbot over all Paul Graham essays, using:

Unstructured.io HTML parser

LangChain Document abstraction

RecursiveCharacterTextSplitter` for chunking

Ollama for embeddings and LLM generation

ChromaDB for local vector retrieval

Similarity + score threshold search

It supports full offline/local inference when using Ollama models.

ðŸ’» Colab Notebook:
<[Colab Notebook](https://colab.research.google.com/gist/abdulrhman186883/e77a7373701bdcaaad96ef26b7f20844/semantic_chunking.ipynb)>

